{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Structure and Hidden State\n",
    "\n",
    "We know that RNNs are used to maintain a kind of memory by linking the output of one node to the input of the next. In the case of an LSTM, for each piece of data in a sequence (say, for a word in a given sentence),\n",
    "there is a corresponding *hidden state* $h_t$. This hidden state is a function of the pieces of data that an LSTM has seen over time; it contains some weights and, essentially, represents a kind of memory for the data that the LSTM has already seen. So, for an LSTM that is looking at words in a sentence, the hidden state of the LSTM will change based on each new word it sees. And, we can use the hidden state to predict the next word in a sequence or help identify the type of word in a language model, and lots of other things!\n",
    "\n",
    "\n",
    "### LSTMs in Pytorch\n",
    "\n",
    "To create and train an LSTM, you have to know how to structure the inputs, and hidden state of an LSTM.\n",
    "\n",
    "In PyTorch, an LSTM expects all of its inputs to be 3D tensors, as follows:\n",
    "* The first axis is the number of sequences of input data (a dimension of 20 could represent 20 input sequences)\n",
    "* The second represents the number of sequences that will be processed in a batch of data (i.e. a dimension of 10 for a batch size of 10 input sequences)\n",
    "* The third represents the number if inputs to process at a time, for example: 1 for one word at a time word\n",
    "\n",
    "These will become clearer in the example in this notebook. This and the following notebook are modified versions of [this PyTorch LSTM tutorial](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html#lstm-s-in-pytorch).\n",
    "\n",
    "Let's take a simple example and say we have a batch_size of 1, for processing one sentence. If we want to run the sequence model over one sentence \"Giraffes in a field\", our input should look like this `1x4` row vector:\n",
    "\n",
    "\\begin{align}\\begin{bmatrix}\n",
    "   \\text{Giraffes  } \n",
    "   \\text{in  } \n",
    "   \\text{a  } \n",
    "   \\text{field} \n",
    "   \\end{bmatrix}\\end{align}\n",
    "\n",
    "With an additional 2nd dimension of size 1 to indicate that this one sentence is one batch.\n",
    "\n",
    "If we wanted to process this sentence one word at a time, the 1st axis will also have a size of 1.\n",
    "\n",
    "Next, let's see an example of one LSTM that is designed to look at a sequence of 4 values (numerical values since those are easiest to create and track) and generate 3 values as output; you are encouraged to change these input/hidden-state sizes to see the effect on the structure of the LSTM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(2) # so that random variables will be consistent and repeatable for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a simple LSTM\n",
    "\n",
    "\n",
    "**A note on hidden and output dimensions**\n",
    "\n",
    "The `hidden_dim` and size of the output will be the same unless you define your own LSTM and change the number of outputs by adding a linear layer at the end of the network, ex. fc = nn.Linear(hidden_dim, output_dim)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define an LSTM with an input dim of 4 and hidden dim of 3\n",
    "# this expects to see 4 values as input and generates 3 values as output\n",
    "input_dim = 4\n",
    "hidden_dim = 3\n",
    "lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim)  \n",
    "\n",
    "# make 5 input sequences of 4 random values each\n",
    "inputs_list = [torch.randn(1, input_dim) for _ in range(5)]\n",
    "print('inputs: \\n', inputs_list)\n",
    "print('\\n')\n",
    "\n",
    "# initialize the hidden state\n",
    "# (1 layer, 1 batch_size, 3 outputs)\n",
    "# first tensor is the hidden state, often called h0\n",
    "# second tensor initializes the cell memory, c0\n",
    "hidden = (torch.randn(1, 1, hidden_dim),\n",
    "          torch.randn(1, 1, hidden_dim))\n",
    "\n",
    "\n",
    "# step through the sequence one element at a time.\n",
    "for i in inputs_list:\n",
    "    # after each step, hidden contains the hidden state\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "    print('out: \\n', out)\n",
    "    print('hidden: \\n', hidden)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that the output and hidden Tensors are always of length 3, which we specified when we defined the LSTM with `hidden_dim`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All at once\n",
    "\n",
    "A for loop is not very efficient for large sequences of data, so we can also, **process all of these inputs at once.**\n",
    "\n",
    "1. concatenate all our input sequences into one big tensor, with a defined batch_size\n",
    "2. define the shape of our hidden state\n",
    "3. get the outputs and the *most recent* hidden state (created after the last word in the sequence has been seen)\n",
    "\n",
    "\n",
    "The outputs may look slightly different due to our differently initialized hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turn inputs into a tensor with 5 rows of data\n",
    "# add the extra 2nd dimension (1) for batch_size\n",
    "inputs = torch.cat(inputs_list).view(len(inputs_list), 1, -1)\n",
    "\n",
    "# print out our inputs and their shape\n",
    "# you should see (number of sequences, batch size, input_dim)\n",
    "print('inputs size: \\n', inputs.size())\n",
    "print('\\n')\n",
    "\n",
    "print('inputs: \\n', inputs)\n",
    "print('\\n')\n",
    "\n",
    "# initialize the hidden state\n",
    "hidden = (torch.randn(1, 1, hidden_dim),\n",
    "          torch.randn(1, 1, hidden_dim))\n",
    "\n",
    "# get the outputs and hidden state\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "\n",
    "print('out: \\n', out)\n",
    "print('hidden: \\n', hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next: Part of Speech\n",
    "\n",
    "Now that you have an understanding of the input and output size for an LSTM, next let's define our own model to tag parts of speech (nouns, verbs, determinants), include an LSTM and a Linear layer to define a desired output size, *and* finally train our model to create a distribution of class scores that associates each input word with a part of speech."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cvnd_1]",
   "language": "python",
   "name": "conda-env-cvnd_1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
